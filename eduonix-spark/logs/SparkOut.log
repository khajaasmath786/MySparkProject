15/11/06 09:59:08 INFO SecurityManager: Changing view acls to: mc41946
15/11/06 09:59:08 INFO SecurityManager: Changing modify acls to: mc41946
15/11/06 09:59:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mc41946); users with modify permissions: Set(mc41946)
15/11/06 09:59:09 INFO Slf4jLogger: Slf4jLogger started
15/11/06 09:59:09 INFO Remoting: Starting remoting
15/11/06 09:59:10 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:57520]
15/11/06 09:59:10 INFO Utils: Successfully started service 'sparkDriver' on port 57520.
15/11/06 09:59:10 INFO SparkEnv: Registering MapOutputTracker
15/11/06 09:59:10 INFO SparkEnv: Registering BlockManagerMaster
15/11/06 09:59:10 INFO DiskBlockManager: Created local directory at C:\Users\mc41946\AppData\Local\Temp\spark-local-20151106095910-007c
15/11/06 09:59:10 INFO MemoryStore: MemoryStore started with capacity 947.7 MB
15/11/06 09:59:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/11/06 09:59:11 INFO HttpFileServer: HTTP File server directory is C:\Users\mc41946\AppData\Local\Temp\spark-da965d99-e1c2-4e9e-a33b-0d333d120653
15/11/06 09:59:11 INFO HttpServer: Starting HTTP Server
15/11/06 09:59:11 INFO Utils: Successfully started service 'HTTP file server' on port 57521.
15/11/06 09:59:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/11/06 09:59:11 INFO SparkUI: Started SparkUI at http://LAP-R9XN5X3.corp.pri:4040
15/11/06 10:04:24 INFO SecurityManager: Changing view acls to: mc41946
15/11/06 10:04:24 INFO SecurityManager: Changing modify acls to: mc41946
15/11/06 10:04:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mc41946); users with modify permissions: Set(mc41946)
15/11/06 10:04:25 INFO Slf4jLogger: Slf4jLogger started
15/11/06 10:04:25 INFO Remoting: Starting remoting
15/11/06 10:04:26 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:57903]
15/11/06 10:04:26 INFO Utils: Successfully started service 'sparkDriver' on port 57903.
15/11/06 10:04:26 INFO SparkEnv: Registering MapOutputTracker
15/11/06 10:04:26 INFO SparkEnv: Registering BlockManagerMaster
15/11/06 10:04:26 INFO DiskBlockManager: Created local directory at C:\Users\mc41946\AppData\Local\Temp\spark-local-20151106100426-18f4
15/11/06 10:04:26 INFO MemoryStore: MemoryStore started with capacity 947.7 MB
15/11/06 10:04:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/11/06 10:04:27 INFO HttpFileServer: HTTP File server directory is C:\Users\mc41946\AppData\Local\Temp\spark-c424a1c3-bed1-4c34-9fe6-5224fc7eeecc
15/11/06 10:04:27 INFO HttpServer: Starting HTTP Server
15/11/06 10:04:27 INFO Utils: Successfully started service 'HTTP file server' on port 57904.
15/11/06 10:04:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/11/06 10:04:27 INFO SparkUI: Started SparkUI at http://LAP-R9XN5X3.corp.pri:4040
15/11/06 10:04:28 INFO SparkContext: Added JAR target/eduonix_spark-deploy.jar at http://192.168.56.1:57904/jars/eduonix_spark-deploy.jar with timestamp 1446825868847
15/11/06 10:04:29 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:57903/user/HeartbeatReceiver
15/11/06 10:04:29 INFO NettyBlockTransferService: Server created on 57923
15/11/06 10:04:29 INFO BlockManagerMaster: Trying to register BlockManager
15/11/06 10:04:29 INFO BlockManagerMasterActor: Registering block manager localhost:57923 with 947.7 MB RAM, BlockManagerId(<driver>, localhost, 57923)
15/11/06 10:04:29 INFO BlockManagerMaster: Registered BlockManager
15/11/06 10:04:30 INFO SparkContext: Starting job: reduce at JavaSparkPi.java:80
15/11/06 10:04:30 INFO DAGScheduler: Got job 0 (reduce at JavaSparkPi.java:80) with 10 output partitions (allowLocal=false)
15/11/06 10:04:30 INFO DAGScheduler: Final stage: Stage 0(reduce at JavaSparkPi.java:80)
15/11/06 10:04:30 INFO DAGScheduler: Parents of final stage: List()
15/11/06 10:04:30 INFO DAGScheduler: Missing parents: List()
15/11/06 10:04:30 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73), which has no missing parents
15/11/06 10:04:31 INFO MemoryStore: ensureFreeSpace(2192) called with curMem=0, maxMem=993735475
15/11/06 10:04:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 947.7 MB)
15/11/06 10:04:31 INFO MemoryStore: ensureFreeSpace(1543) called with curMem=2192, maxMem=993735475
15/11/06 10:04:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1543.0 B, free 947.7 MB)
15/11/06 10:04:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:57923 (size: 1543.0 B, free: 947.7 MB)
15/11/06 10:04:31 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/11/06 10:04:31 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:838
15/11/06 10:04:31 INFO DAGScheduler: Submitting 10 missing tasks from Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73)
15/11/06 10:04:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks
15/11/06 10:04:31 WARN TaskSetManager: Stage 0 contains a task of very large size (977 KB). The maximum recommended task size is 100 KB.
15/11/06 10:04:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 10:04:31 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 10:04:31 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/11/06 10:04:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/11/06 10:04:31 INFO Executor: Fetching http://192.168.56.1:57904/jars/eduonix_spark-deploy.jar with timestamp 1446825868847
15/11/06 10:04:31 INFO Utils: Fetching http://192.168.56.1:57904/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp9085350057936451763.tmp
15/11/06 10:04:34 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:867)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 10:04:34 INFO Executor: Fetching http://192.168.56.1:57904/jars/eduonix_spark-deploy.jar with timestamp 1446825868847
15/11/06 10:04:34 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 10:04:34 INFO Utils: Fetching http://192.168.56.1:57904/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp158549710739230079.tmp
15/11/06 10:04:34 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 10:04:34 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/11/06 10:04:34 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

15/11/06 10:04:34 ERROR TaskSetManager: Task 1 in stage 0.0 failed 1 times; aborting job
15/11/06 10:04:34 INFO TaskSchedulerImpl: Cancelling stage 0
15/11/06 10:04:34 INFO TaskSchedulerImpl: Stage 0 was cancelled
15/11/06 10:04:34 INFO Executor: Executor is trying to kill task 0.0 in stage 0.0 (TID 0)
15/11/06 10:04:34 INFO Executor: Executor is trying to kill task 2.0 in stage 0.0 (TID 2)
15/11/06 10:04:34 INFO DAGScheduler: Job 0 failed: reduce at JavaSparkPi.java:80, took 4.302868 s
15/11/06 10:04:34 ERROR Utils: Exception while deleting Spark temp dir: C:\Users\mc41946\AppData\Local\Temp\spark-c424a1c3-bed1-4c34-9fe6-5224fc7eeecc
java.io.IOException: Failed to delete: C:\Users\mc41946\AppData\Local\Temp\spark-c424a1c3-bed1-4c34-9fe6-5224fc7eeecc
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:782)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1$$anonfun$apply$mcV$sp$2.apply(Utils.scala:177)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1$$anonfun$apply$mcV$sp$2.apply(Utils.scala:175)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply$mcV$sp(Utils.scala:175)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:173)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:173)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1460)
	at org.apache.spark.util.Utils$$anon$4.run(Utils.scala:173)
15/11/06 11:42:59 INFO SecurityManager: Changing view acls to: mc41946
15/11/06 11:42:59 INFO SecurityManager: Changing modify acls to: mc41946
15/11/06 11:42:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mc41946); users with modify permissions: Set(mc41946)
15/11/06 11:42:59 INFO Slf4jLogger: Slf4jLogger started
15/11/06 11:43:00 INFO Remoting: Starting remoting
15/11/06 11:43:00 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:56381]
15/11/06 11:43:00 INFO Utils: Successfully started service 'sparkDriver' on port 56381.
15/11/06 11:43:00 INFO SparkEnv: Registering MapOutputTracker
15/11/06 11:43:00 INFO SparkEnv: Registering BlockManagerMaster
15/11/06 11:43:00 INFO DiskBlockManager: Created local directory at C:\Users\mc41946\AppData\Local\Temp\spark-local-20151106114300-c062
15/11/06 11:43:00 INFO MemoryStore: MemoryStore started with capacity 947.7 MB
15/11/06 11:43:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/11/06 11:43:01 INFO HttpFileServer: HTTP File server directory is C:\Users\mc41946\AppData\Local\Temp\spark-c4112412-09b2-4be3-8811-c88ed865ca36
15/11/06 11:43:01 INFO HttpServer: Starting HTTP Server
15/11/06 11:43:01 INFO Utils: Successfully started service 'HTTP file server' on port 56382.
15/11/06 11:43:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/11/06 11:43:02 INFO SparkUI: Started SparkUI at http://LAP-R9XN5X3.corp.pri:4040
15/11/06 11:43:03 INFO SparkContext: Added JAR target/eduonix_spark-deploy.jar at http://192.168.56.1:56382/jars/eduonix_spark-deploy.jar with timestamp 1446831783868
15/11/06 11:43:04 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:56381/user/HeartbeatReceiver
15/11/06 11:43:04 INFO NettyBlockTransferService: Server created on 56401
15/11/06 11:43:04 INFO BlockManagerMaster: Trying to register BlockManager
15/11/06 11:43:04 INFO BlockManagerMasterActor: Registering block manager localhost:56401 with 947.7 MB RAM, BlockManagerId(<driver>, localhost, 56401)
15/11/06 11:43:04 INFO BlockManagerMaster: Registered BlockManager
15/11/06 11:43:05 INFO SparkContext: Starting job: reduce at JavaSparkPi.java:80
15/11/06 11:43:05 INFO DAGScheduler: Got job 0 (reduce at JavaSparkPi.java:80) with 10 output partitions (allowLocal=false)
15/11/06 11:43:05 INFO DAGScheduler: Final stage: Stage 0(reduce at JavaSparkPi.java:80)
15/11/06 11:43:05 INFO DAGScheduler: Parents of final stage: List()
15/11/06 11:43:05 INFO DAGScheduler: Missing parents: List()
15/11/06 11:43:05 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73), which has no missing parents
15/11/06 11:43:05 INFO MemoryStore: ensureFreeSpace(2192) called with curMem=0, maxMem=993735475
15/11/06 11:43:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 947.7 MB)
15/11/06 11:43:06 INFO MemoryStore: ensureFreeSpace(1541) called with curMem=2192, maxMem=993735475
15/11/06 11:43:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1541.0 B, free 947.7 MB)
15/11/06 11:43:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:56401 (size: 1541.0 B, free: 947.7 MB)
15/11/06 11:43:06 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/11/06 11:43:06 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:838
15/11/06 11:43:06 INFO DAGScheduler: Submitting 10 missing tasks from Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73)
15/11/06 11:43:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks
15/11/06 11:43:06 WARN TaskSetManager: Stage 0 contains a task of very large size (977 KB). The maximum recommended task size is 100 KB.
15/11/06 11:43:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 11:43:06 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 11:43:06 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/11/06 11:43:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/11/06 11:43:06 INFO Executor: Fetching http://192.168.56.1:56382/jars/eduonix_spark-deploy.jar with timestamp 1446831783868
15/11/06 11:43:06 INFO Utils: Fetching http://192.168.56.1:56382/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp5382915448030063895.tmp
15/11/06 11:43:09 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:867)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 11:43:09 INFO Executor: Fetching http://192.168.56.1:56382/jars/eduonix_spark-deploy.jar with timestamp 1446831783868
15/11/06 11:43:09 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 11:43:09 INFO Utils: Fetching http://192.168.56.1:56382/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp4710818195076702123.tmp
15/11/06 11:43:09 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 11:43:09 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/11/06 11:43:09 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

15/11/06 11:43:09 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
15/11/06 11:43:09 INFO TaskSchedulerImpl: Cancelling stage 0
15/11/06 11:43:09 INFO Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1)
15/11/06 11:43:09 INFO Executor: Executor is trying to kill task 2.0 in stage 0.0 (TID 2)
15/11/06 11:43:09 INFO TaskSchedulerImpl: Stage 0 was cancelled
15/11/06 11:43:09 INFO DAGScheduler: Job 0 failed: reduce at JavaSparkPi.java:80, took 3.756074 s
15/11/06 11:43:09 ERROR Utils: Exception while deleting Spark temp dir: C:\Users\mc41946\AppData\Local\Temp\spark-c4112412-09b2-4be3-8811-c88ed865ca36
java.io.IOException: Failed to delete: C:\Users\mc41946\AppData\Local\Temp\spark-c4112412-09b2-4be3-8811-c88ed865ca36
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:782)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1$$anonfun$apply$mcV$sp$2.apply(Utils.scala:177)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1$$anonfun$apply$mcV$sp$2.apply(Utils.scala:175)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply$mcV$sp(Utils.scala:175)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:173)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:173)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1460)
	at org.apache.spark.util.Utils$$anon$4.run(Utils.scala:173)
15/11/06 11:47:04 INFO SecurityManager: Changing view acls to: mc41946
15/11/06 11:47:04 INFO SecurityManager: Changing modify acls to: mc41946
15/11/06 11:47:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mc41946); users with modify permissions: Set(mc41946)
15/11/06 11:47:05 INFO Slf4jLogger: Slf4jLogger started
15/11/06 11:47:05 INFO Remoting: Starting remoting
15/11/06 11:47:06 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:56619]
15/11/06 11:47:06 INFO Utils: Successfully started service 'sparkDriver' on port 56619.
15/11/06 11:47:06 INFO SparkEnv: Registering MapOutputTracker
15/11/06 11:47:06 INFO SparkEnv: Registering BlockManagerMaster
15/11/06 11:47:06 INFO DiskBlockManager: Created local directory at C:\Users\mc41946\AppData\Local\Temp\spark-local-20151106114706-996f
15/11/06 11:47:06 INFO MemoryStore: MemoryStore started with capacity 947.7 MB
15/11/06 11:47:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/11/06 11:47:07 INFO HttpFileServer: HTTP File server directory is C:\Users\mc41946\AppData\Local\Temp\spark-b225b576-c88f-4db9-bd8a-846e4468aff8
15/11/06 11:47:07 INFO HttpServer: Starting HTTP Server
15/11/06 11:47:07 INFO Utils: Successfully started service 'HTTP file server' on port 56620.
15/11/06 11:47:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/11/06 11:47:07 INFO SparkUI: Started SparkUI at http://LAP-R9XN5X3.corp.pri:4040
15/11/06 11:47:08 INFO SparkContext: Added JAR target/eduonix_spark-deploy.jar at http://192.168.56.1:56620/jars/eduonix_spark-deploy.jar with timestamp 1446832028468
15/11/06 11:47:08 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:56619/user/HeartbeatReceiver
15/11/06 11:47:09 INFO NettyBlockTransferService: Server created on 56639
15/11/06 11:47:09 INFO BlockManagerMaster: Trying to register BlockManager
15/11/06 11:47:09 INFO BlockManagerMasterActor: Registering block manager localhost:56639 with 947.7 MB RAM, BlockManagerId(<driver>, localhost, 56639)
15/11/06 11:47:09 INFO BlockManagerMaster: Registered BlockManager
15/11/06 11:47:09 INFO SparkContext: Starting job: reduce at JavaSparkPi.java:80
15/11/06 11:47:09 INFO DAGScheduler: Got job 0 (reduce at JavaSparkPi.java:80) with 10 output partitions (allowLocal=false)
15/11/06 11:47:09 INFO DAGScheduler: Final stage: Stage 0(reduce at JavaSparkPi.java:80)
15/11/06 11:47:09 INFO DAGScheduler: Parents of final stage: List()
15/11/06 11:47:09 INFO DAGScheduler: Missing parents: List()
15/11/06 11:47:09 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73), which has no missing parents
15/11/06 11:47:09 INFO MemoryStore: ensureFreeSpace(2192) called with curMem=0, maxMem=993735475
15/11/06 11:47:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 947.7 MB)
15/11/06 11:47:10 INFO MemoryStore: ensureFreeSpace(1541) called with curMem=2192, maxMem=993735475
15/11/06 11:47:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1541.0 B, free 947.7 MB)
15/11/06 11:47:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:56639 (size: 1541.0 B, free: 947.7 MB)
15/11/06 11:47:10 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/11/06 11:47:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:838
15/11/06 11:47:10 INFO DAGScheduler: Submitting 10 missing tasks from Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73)
15/11/06 11:47:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks
15/11/06 11:47:10 WARN TaskSetManager: Stage 0 contains a task of very large size (977 KB). The maximum recommended task size is 100 KB.
15/11/06 11:47:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 11:47:10 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 11:47:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/11/06 11:47:10 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/11/06 11:47:10 INFO Executor: Fetching http://192.168.56.1:56620/jars/eduonix_spark-deploy.jar with timestamp 1446832028468
15/11/06 11:47:10 INFO Utils: Fetching http://192.168.56.1:56620/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp2659353636090525140.tmp
15/11/06 11:47:12 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:867)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 11:47:12 INFO Executor: Fetching http://192.168.56.1:56620/jars/eduonix_spark-deploy.jar with timestamp 1446832028468
15/11/06 11:47:12 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 11:47:12 INFO Utils: Fetching http://192.168.56.1:56620/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp9173820394524694483.tmp
15/11/06 11:47:12 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 11:47:12 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/11/06 11:47:12 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

15/11/06 11:47:12 ERROR TaskSetManager: Task 1 in stage 0.0 failed 1 times; aborting job
15/11/06 11:47:12 INFO TaskSchedulerImpl: Cancelling stage 0
15/11/06 11:47:12 INFO TaskSchedulerImpl: Stage 0 was cancelled
15/11/06 11:47:12 INFO Executor: Executor is trying to kill task 0.0 in stage 0.0 (TID 0)
15/11/06 11:47:12 INFO Executor: Executor is trying to kill task 2.0 in stage 0.0 (TID 2)
15/11/06 11:47:12 INFO DAGScheduler: Job 0 failed: reduce at JavaSparkPi.java:80, took 3.015846 s
15/11/06 11:47:12 ERROR Utils: Exception while deleting Spark temp dir: C:\Users\mc41946\AppData\Local\Temp\spark-b225b576-c88f-4db9-bd8a-846e4468aff8
java.io.IOException: Failed to delete: C:\Users\mc41946\AppData\Local\Temp\spark-b225b576-c88f-4db9-bd8a-846e4468aff8
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:782)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1$$anonfun$apply$mcV$sp$2.apply(Utils.scala:177)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1$$anonfun$apply$mcV$sp$2.apply(Utils.scala:175)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply$mcV$sp(Utils.scala:175)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:173)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:173)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1460)
	at org.apache.spark.util.Utils$$anon$4.run(Utils.scala:173)
15/11/06 12:00:20 INFO SecurityManager: Changing view acls to: mc41946
15/11/06 12:00:20 INFO SecurityManager: Changing modify acls to: mc41946
15/11/06 12:00:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mc41946); users with modify permissions: Set(mc41946)
15/11/06 12:00:20 INFO Slf4jLogger: Slf4jLogger started
15/11/06 12:00:20 INFO Remoting: Starting remoting
15/11/06 12:00:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:56837]
15/11/06 12:00:20 INFO Utils: Successfully started service 'sparkDriver' on port 56837.
15/11/06 12:00:20 INFO SparkEnv: Registering MapOutputTracker
15/11/06 12:00:20 INFO SparkEnv: Registering BlockManagerMaster
15/11/06 12:00:20 INFO DiskBlockManager: Created local directory at C:\Users\mc41946\AppData\Local\Temp\spark-local-20151106120020-fbbb
15/11/06 12:00:20 INFO MemoryStore: MemoryStore started with capacity 947.7 MB
15/11/06 12:00:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/11/06 12:00:21 INFO HttpFileServer: HTTP File server directory is C:\Users\mc41946\AppData\Local\Temp\spark-f9482389-dcaf-4579-babb-18f6063bf394
15/11/06 12:00:21 INFO HttpServer: Starting HTTP Server
15/11/06 12:00:21 INFO Utils: Successfully started service 'HTTP file server' on port 56840.
15/11/06 12:00:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/11/06 12:00:21 INFO SparkUI: Started SparkUI at http://LAP-R9XN5X3.corp.pri:4040
15/11/06 12:00:23 INFO SparkContext: Added JAR target/eduonix_spark-deploy.jar at http://192.168.56.1:56840/jars/eduonix_spark-deploy.jar with timestamp 1446832823732
15/11/06 12:00:23 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:56837/user/HeartbeatReceiver
15/11/06 12:00:24 INFO NettyBlockTransferService: Server created on 56861
15/11/06 12:00:24 INFO BlockManagerMaster: Trying to register BlockManager
15/11/06 12:00:24 INFO BlockManagerMasterActor: Registering block manager localhost:56861 with 947.7 MB RAM, BlockManagerId(<driver>, localhost, 56861)
15/11/06 12:00:24 INFO BlockManagerMaster: Registered BlockManager
15/11/06 12:00:25 INFO SparkContext: Starting job: reduce at JavaSparkPi.java:80
15/11/06 12:00:25 INFO DAGScheduler: Got job 0 (reduce at JavaSparkPi.java:80) with 10 output partitions (allowLocal=false)
15/11/06 12:00:25 INFO DAGScheduler: Final stage: Stage 0(reduce at JavaSparkPi.java:80)
15/11/06 12:00:25 INFO DAGScheduler: Parents of final stage: List()
15/11/06 12:00:25 INFO DAGScheduler: Missing parents: List()
15/11/06 12:00:25 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73), which has no missing parents
15/11/06 12:00:25 INFO MemoryStore: ensureFreeSpace(2192) called with curMem=0, maxMem=993735475
15/11/06 12:00:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 947.7 MB)
15/11/06 12:00:25 INFO MemoryStore: ensureFreeSpace(1541) called with curMem=2192, maxMem=993735475
15/11/06 12:00:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1541.0 B, free 947.7 MB)
15/11/06 12:00:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:56861 (size: 1541.0 B, free: 947.7 MB)
15/11/06 12:00:25 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/11/06 12:00:25 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:838
15/11/06 12:00:25 INFO DAGScheduler: Submitting 10 missing tasks from Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73)
15/11/06 12:00:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks
15/11/06 12:00:25 WARN TaskSetManager: Stage 0 contains a task of very large size (977 KB). The maximum recommended task size is 100 KB.
15/11/06 12:00:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 12:00:26 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 12:00:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/11/06 12:00:26 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/11/06 12:00:26 INFO Executor: Fetching http://192.168.56.1:56840/jars/eduonix_spark-deploy.jar with timestamp 1446832823732
15/11/06 12:00:26 INFO Utils: Fetching http://192.168.56.1:56840/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp6593978021812885062.tmp
15/11/06 12:00:28 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:867)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 12:00:28 INFO Executor: Fetching http://192.168.56.1:56840/jars/eduonix_spark-deploy.jar with timestamp 1446832823732
15/11/06 12:00:28 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 12:00:28 INFO Utils: Fetching http://192.168.56.1:56840/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp7145850868082461065.tmp
15/11/06 12:00:28 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 12:00:28 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/11/06 12:00:28 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

15/11/06 12:00:28 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
15/11/06 12:00:28 INFO TaskSchedulerImpl: Cancelling stage 0
15/11/06 12:00:28 INFO Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1)
15/11/06 12:00:28 INFO TaskSchedulerImpl: Stage 0 was cancelled
15/11/06 12:00:28 INFO Executor: Executor is trying to kill task 2.0 in stage 0.0 (TID 2)
15/11/06 12:00:28 INFO DAGScheduler: Job 0 failed: reduce at JavaSparkPi.java:80, took 3.192240 s
15/11/06 12:00:29 WARN AbstractHttpConnection: /jars/eduonix_spark-deploy.jar
java.io.FileNotFoundException: C:\Users\mc41946\AppData\Local\Temp\spark-f9482389-dcaf-4579-babb-18f6063bf394\jars\eduonix_spark-deploy.jar (Access is denied)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(Unknown Source)
	at org.eclipse.jetty.util.resource.FileResource.getInputStream(FileResource.java:286)
	at org.eclipse.jetty.server.handler.ResourceHandler.handle(ResourceHandler.java:487)
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)
	at org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Unknown Source)
15/11/06 12:00:29 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.io.IOException: Server returned HTTP response code: 500 for URL: http://192.168.56.1:56840/jars/eduonix_spark-deploy.jar
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(Unknown Source)
	at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:452)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:399)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 12:00:29 INFO Executor: Fetching http://192.168.56.1:56840/jars/eduonix_spark-deploy.jar with timestamp 1446832823732
15/11/06 12:00:29 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.io.IOException: Server returned HTTP response code: 500 for URL: http://192.168.56.1:56840/jars/eduonix_spark-deploy.jar
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(Unknown Source)
	at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:452)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:399)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

15/11/06 12:00:29 INFO Utils: Fetching http://192.168.56.1:56840/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp2229055251696370029.tmp
15/11/06 12:00:29 ERROR Executor: Exception in task 2.0 in stage 0.0 (TID 2)
java.io.FileNotFoundException: http://192.168.56.1:56840/jars/eduonix_spark-deploy.jar
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(Unknown Source)
	at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:452)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:399)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 12:20:51 INFO SecurityManager: Changing view acls to: mc41946
15/11/06 12:20:51 INFO SecurityManager: Changing modify acls to: mc41946
15/11/06 12:20:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mc41946); users with modify permissions: Set(mc41946)
15/11/06 12:20:51 INFO Slf4jLogger: Slf4jLogger started
15/11/06 12:20:51 INFO Remoting: Starting remoting
15/11/06 12:20:51 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:57195]
15/11/06 12:20:51 INFO Utils: Successfully started service 'sparkDriver' on port 57195.
15/11/06 12:20:52 INFO SparkEnv: Registering MapOutputTracker
15/11/06 12:20:52 INFO SparkEnv: Registering BlockManagerMaster
15/11/06 12:20:52 INFO DiskBlockManager: Created local directory at C:\Users\mc41946\AppData\Local\Temp\spark-local-20151106122052-fade
15/11/06 12:20:52 INFO MemoryStore: MemoryStore started with capacity 947.7 MB
15/11/06 12:20:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/11/06 12:20:52 INFO HttpFileServer: HTTP File server directory is C:\Users\mc41946\AppData\Local\Temp\spark-e173fa43-c840-4a22-aa4f-97ded6bddf8d
15/11/06 12:20:52 INFO HttpServer: Starting HTTP Server
15/11/06 12:20:52 INFO Utils: Successfully started service 'HTTP file server' on port 57196.
15/11/06 12:20:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/11/06 12:20:52 INFO SparkUI: Started SparkUI at http://LAP-R9XN5X3.corp.pri:4040
15/11/06 12:20:53 INFO SparkContext: Added JAR target/eduonix_spark-deploy.jar at http://192.168.56.1:57196/jars/eduonix_spark-deploy.jar with timestamp 1446834053844
15/11/06 12:20:53 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:57195/user/HeartbeatReceiver
15/11/06 12:20:54 INFO NettyBlockTransferService: Server created on 57215
15/11/06 12:20:54 INFO BlockManagerMaster: Trying to register BlockManager
15/11/06 12:20:54 INFO BlockManagerMasterActor: Registering block manager localhost:57215 with 947.7 MB RAM, BlockManagerId(<driver>, localhost, 57215)
15/11/06 12:20:54 INFO BlockManagerMaster: Registered BlockManager
15/11/06 12:20:54 INFO SparkContext: Starting job: reduce at JavaSparkPi.java:80
15/11/06 12:20:54 INFO DAGScheduler: Got job 0 (reduce at JavaSparkPi.java:80) with 10 output partitions (allowLocal=false)
15/11/06 12:20:54 INFO DAGScheduler: Final stage: Stage 0(reduce at JavaSparkPi.java:80)
15/11/06 12:20:54 INFO DAGScheduler: Parents of final stage: List()
15/11/06 12:20:54 INFO DAGScheduler: Missing parents: List()
15/11/06 12:20:54 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73), which has no missing parents
15/11/06 12:20:54 INFO MemoryStore: ensureFreeSpace(2192) called with curMem=0, maxMem=993735475
15/11/06 12:20:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 947.7 MB)
15/11/06 12:20:54 INFO MemoryStore: ensureFreeSpace(1541) called with curMem=2192, maxMem=993735475
15/11/06 12:20:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1541.0 B, free 947.7 MB)
15/11/06 12:20:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:57215 (size: 1541.0 B, free: 947.7 MB)
15/11/06 12:20:54 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/11/06 12:20:54 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:838
15/11/06 12:20:55 INFO DAGScheduler: Submitting 10 missing tasks from Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73)
15/11/06 12:20:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks
15/11/06 12:20:55 WARN TaskSetManager: Stage 0 contains a task of very large size (977 KB). The maximum recommended task size is 100 KB.
15/11/06 12:20:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 12:20:55 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 12:20:55 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/11/06 12:20:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/11/06 12:20:55 INFO Executor: Fetching http://192.168.56.1:57196/jars/eduonix_spark-deploy.jar with timestamp 1446834053844
15/11/06 12:20:55 INFO Utils: Fetching http://192.168.56.1:57196/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp1964141197173493550.tmp
15/11/06 12:20:56 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:867)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 12:20:56 INFO Executor: Fetching http://192.168.56.1:57196/jars/eduonix_spark-deploy.jar with timestamp 1446834053844
15/11/06 12:20:56 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 12:20:56 INFO Utils: Fetching http://192.168.56.1:57196/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp1878689332982241435.tmp
15/11/06 12:20:56 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 12:20:56 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/11/06 12:20:56 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

15/11/06 12:20:56 ERROR TaskSetManager: Task 1 in stage 0.0 failed 1 times; aborting job
15/11/06 12:20:56 INFO TaskSchedulerImpl: Cancelling stage 0
15/11/06 12:20:56 INFO TaskSchedulerImpl: Stage 0 was cancelled
15/11/06 12:20:56 INFO Executor: Executor is trying to kill task 0.0 in stage 0.0 (TID 0)
15/11/06 12:20:56 INFO Executor: Executor is trying to kill task 2.0 in stage 0.0 (TID 2)
15/11/06 12:20:56 INFO DAGScheduler: Job 0 failed: reduce at JavaSparkPi.java:80, took 2.257390 s
15/11/06 12:20:57 ERROR Utils: Exception while deleting Spark temp dir: C:\Users\mc41946\AppData\Local\Temp\spark-e173fa43-c840-4a22-aa4f-97ded6bddf8d
java.io.IOException: Failed to delete: C:\Users\mc41946\AppData\Local\Temp\spark-e173fa43-c840-4a22-aa4f-97ded6bddf8d
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:782)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1$$anonfun$apply$mcV$sp$2.apply(Utils.scala:177)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1$$anonfun$apply$mcV$sp$2.apply(Utils.scala:175)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply$mcV$sp(Utils.scala:175)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:173)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:173)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1460)
	at org.apache.spark.util.Utils$$anon$4.run(Utils.scala:173)
15/11/06 12:29:32 INFO SecurityManager: Changing view acls to: mc41946
15/11/06 12:29:32 INFO SecurityManager: Changing modify acls to: mc41946
15/11/06 12:29:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mc41946); users with modify permissions: Set(mc41946)
15/11/06 12:29:33 INFO Slf4jLogger: Slf4jLogger started
15/11/06 12:29:34 INFO Remoting: Starting remoting
15/11/06 12:29:34 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:57303]
15/11/06 12:29:34 INFO Utils: Successfully started service 'sparkDriver' on port 57303.
15/11/06 12:29:34 INFO SparkEnv: Registering MapOutputTracker
15/11/06 12:29:34 INFO SparkEnv: Registering BlockManagerMaster
15/11/06 12:29:34 INFO DiskBlockManager: Created local directory at C:\Users\mc41946\AppData\Local\Temp\spark-local-20151106122934-9feb
15/11/06 12:29:34 INFO MemoryStore: MemoryStore started with capacity 947.7 MB
15/11/06 12:29:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/11/06 12:29:35 INFO HttpFileServer: HTTP File server directory is C:\Users\mc41946\AppData\Local\Temp\spark-53869bfd-d2d8-4524-a583-1847c8b95bae
15/11/06 12:29:35 INFO HttpServer: Starting HTTP Server
15/11/06 12:29:35 INFO Utils: Successfully started service 'HTTP file server' on port 57304.
15/11/06 12:29:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/11/06 12:29:35 INFO SparkUI: Started SparkUI at http://LAP-R9XN5X3.corp.pri:4040
15/11/06 12:29:37 INFO SparkContext: Added JAR target/eduonix_spark-deploy.jar at http://192.168.56.1:57304/jars/eduonix_spark-deploy.jar with timestamp 1446834577579
15/11/06 12:29:37 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:57303/user/HeartbeatReceiver
15/11/06 12:29:38 INFO NettyBlockTransferService: Server created on 57325
15/11/06 12:29:38 INFO BlockManagerMaster: Trying to register BlockManager
15/11/06 12:29:38 INFO BlockManagerMasterActor: Registering block manager localhost:57325 with 947.7 MB RAM, BlockManagerId(<driver>, localhost, 57325)
15/11/06 12:29:38 INFO BlockManagerMaster: Registered BlockManager
15/11/06 12:29:38 INFO SparkContext: Starting job: reduce at JavaSparkPi.java:80
15/11/06 12:29:38 INFO DAGScheduler: Got job 0 (reduce at JavaSparkPi.java:80) with 10 output partitions (allowLocal=false)
15/11/06 12:29:38 INFO DAGScheduler: Final stage: Stage 0(reduce at JavaSparkPi.java:80)
15/11/06 12:29:38 INFO DAGScheduler: Parents of final stage: List()
15/11/06 12:29:38 INFO DAGScheduler: Missing parents: List()
15/11/06 12:29:38 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73), which has no missing parents
15/11/06 12:29:38 INFO MemoryStore: ensureFreeSpace(2192) called with curMem=0, maxMem=993735475
15/11/06 12:29:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 947.7 MB)
15/11/06 12:29:38 INFO MemoryStore: ensureFreeSpace(1541) called with curMem=2192, maxMem=993735475
15/11/06 12:29:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1541.0 B, free 947.7 MB)
15/11/06 12:29:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:57325 (size: 1541.0 B, free: 947.7 MB)
15/11/06 12:29:38 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/11/06 12:29:38 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:838
15/11/06 12:29:39 INFO DAGScheduler: Submitting 10 missing tasks from Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73)
15/11/06 12:29:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks
15/11/06 12:29:39 WARN TaskSetManager: Stage 0 contains a task of very large size (977 KB). The maximum recommended task size is 100 KB.
15/11/06 12:29:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 12:29:39 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 12:29:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/11/06 12:29:39 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/11/06 12:29:39 INFO Executor: Fetching http://192.168.56.1:57304/jars/eduonix_spark-deploy.jar with timestamp 1446834577579
15/11/06 12:29:39 INFO Utils: Fetching http://192.168.56.1:57304/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp3527849579533815345.tmp
15/11/06 12:29:41 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:867)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 12:29:41 INFO Executor: Fetching http://192.168.56.1:57304/jars/eduonix_spark-deploy.jar with timestamp 1446834577579
15/11/06 12:29:41 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 12:29:41 INFO Utils: Fetching http://192.168.56.1:57304/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp7712989921185677685.tmp
15/11/06 12:29:41 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1001394 bytes)
15/11/06 12:29:41 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/11/06 12:29:41 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

15/11/06 12:29:41 ERROR TaskSetManager: Task 1 in stage 0.0 failed 1 times; aborting job
15/11/06 12:29:41 INFO TaskSchedulerImpl: Cancelling stage 0
15/11/06 12:29:41 INFO Executor: Executor is trying to kill task 0.0 in stage 0.0 (TID 0)
15/11/06 12:29:41 INFO Executor: Executor is trying to kill task 2.0 in stage 0.0 (TID 2)
15/11/06 12:29:41 INFO TaskSchedulerImpl: Stage 0 was cancelled
15/11/06 12:29:41 INFO DAGScheduler: Job 0 failed: reduce at JavaSparkPi.java:80, took 2.797010 s
15/11/06 12:29:41 ERROR Utils: Exception while deleting Spark temp dir: C:\Users\mc41946\AppData\Local\Temp\spark-53869bfd-d2d8-4524-a583-1847c8b95bae
java.io.IOException: Failed to delete: C:\Users\mc41946\AppData\Local\Temp\spark-53869bfd-d2d8-4524-a583-1847c8b95bae
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:782)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1$$anonfun$apply$mcV$sp$2.apply(Utils.scala:177)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1$$anonfun$apply$mcV$sp$2.apply(Utils.scala:175)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply$mcV$sp(Utils.scala:175)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:173)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:173)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1460)
	at org.apache.spark.util.Utils$$anon$4.run(Utils.scala:173)
15/11/06 15:30:04 INFO SecurityManager: Changing view acls to: mc41946
15/11/06 15:30:04 INFO SecurityManager: Changing modify acls to: mc41946
15/11/06 15:30:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mc41946); users with modify permissions: Set(mc41946)
15/11/06 15:30:05 INFO Slf4jLogger: Slf4jLogger started
15/11/06 15:30:05 INFO Remoting: Starting remoting
15/11/06 15:30:05 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:60849]
15/11/06 15:30:05 INFO Utils: Successfully started service 'sparkDriver' on port 60849.
15/11/06 15:30:05 INFO SparkEnv: Registering MapOutputTracker
15/11/06 15:30:05 INFO SparkEnv: Registering BlockManagerMaster
15/11/06 15:30:05 INFO DiskBlockManager: Created local directory at C:\Users\mc41946\AppData\Local\Temp\spark-local-20151106153005-74d5
15/11/06 15:30:05 INFO MemoryStore: MemoryStore started with capacity 947.7 MB
15/11/06 15:30:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/11/06 15:30:06 INFO HttpFileServer: HTTP File server directory is C:\Users\mc41946\AppData\Local\Temp\spark-20fc99e1-7bfe-4962-be7b-c1c7a3828ed4
15/11/06 15:30:06 INFO HttpServer: Starting HTTP Server
15/11/06 15:30:06 INFO Utils: Successfully started service 'HTTP file server' on port 60850.
15/11/06 15:30:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/11/06 15:30:06 INFO SparkUI: Started SparkUI at http://LAP-R9XN5X3.corp.pri:4040
15/11/06 15:30:07 INFO SparkContext: Added JAR target/eduonix_spark-deploy.jar at http://10.0.0.8:60850/jars/eduonix_spark-deploy.jar with timestamp 1446845407896
15/11/06 15:30:08 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:60849/user/HeartbeatReceiver
15/11/06 15:30:08 INFO NettyBlockTransferService: Server created on 60869
15/11/06 15:30:08 INFO BlockManagerMaster: Trying to register BlockManager
15/11/06 15:30:08 INFO BlockManagerMasterActor: Registering block manager localhost:60869 with 947.7 MB RAM, BlockManagerId(<driver>, localhost, 60869)
15/11/06 15:30:08 INFO BlockManagerMaster: Registered BlockManager
15/11/06 15:30:09 INFO SparkContext: Starting job: reduce at JavaSparkPi.java:80
15/11/06 15:30:09 INFO DAGScheduler: Got job 0 (reduce at JavaSparkPi.java:80) with 10 output partitions (allowLocal=false)
15/11/06 15:30:09 INFO DAGScheduler: Final stage: Stage 0(reduce at JavaSparkPi.java:80)
15/11/06 15:30:09 INFO DAGScheduler: Parents of final stage: List()
15/11/06 15:30:09 INFO DAGScheduler: Missing parents: List()
15/11/06 15:30:09 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73), which has no missing parents
15/11/06 15:30:10 INFO MemoryStore: ensureFreeSpace(2192) called with curMem=0, maxMem=993735475
15/11/06 15:30:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 947.7 MB)
15/11/06 15:30:10 INFO MemoryStore: ensureFreeSpace(1541) called with curMem=2192, maxMem=993735475
15/11/06 15:30:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1541.0 B, free 947.7 MB)
15/11/06 15:30:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:60869 (size: 1541.0 B, free: 947.7 MB)
15/11/06 15:30:10 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/11/06 15:30:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:838
15/11/06 15:30:10 INFO DAGScheduler: Submitting 10 missing tasks from Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73)
15/11/06 15:30:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks
15/11/06 15:30:11 WARN TaskSetManager: Stage 0 contains a task of very large size (977 KB). The maximum recommended task size is 100 KB.
15/11/06 15:30:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1001390 bytes)
15/11/06 15:30:11 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1001390 bytes)
15/11/06 15:30:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/11/06 15:30:11 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/11/06 15:30:11 INFO Executor: Fetching http://10.0.0.8:60850/jars/eduonix_spark-deploy.jar with timestamp 1446845407896
15/11/06 15:30:11 INFO Utils: Fetching http://10.0.0.8:60850/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp3956702604116887458.tmp
15/11/06 15:30:14 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:867)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 15:30:14 INFO Executor: Fetching http://10.0.0.8:60850/jars/eduonix_spark-deploy.jar with timestamp 1446845407896
15/11/06 15:30:14 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 15:30:14 INFO Utils: Fetching http://10.0.0.8:60850/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp3057065604930430536.tmp
15/11/06 15:30:14 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1001390 bytes)
15/11/06 15:30:14 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/11/06 15:30:14 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

15/11/06 15:30:14 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
15/11/06 15:30:14 INFO TaskSchedulerImpl: Cancelling stage 0
15/11/06 15:30:14 INFO Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1)
15/11/06 15:30:14 INFO Executor: Executor is trying to kill task 2.0 in stage 0.0 (TID 2)
15/11/06 15:30:14 INFO TaskSchedulerImpl: Stage 0 was cancelled
15/11/06 15:30:14 INFO DAGScheduler: Job 0 failed: reduce at JavaSparkPi.java:80, took 4.348975 s
15/11/06 15:30:14 ERROR Utils: Exception while deleting Spark temp dir: C:\Users\mc41946\AppData\Local\Temp\spark-20fc99e1-7bfe-4962-be7b-c1c7a3828ed4
java.io.IOException: Failed to delete: C:\Users\mc41946\AppData\Local\Temp\spark-20fc99e1-7bfe-4962-be7b-c1c7a3828ed4
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:782)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1$$anonfun$apply$mcV$sp$2.apply(Utils.scala:177)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1$$anonfun$apply$mcV$sp$2.apply(Utils.scala:175)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply$mcV$sp(Utils.scala:175)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:173)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:173)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1460)
	at org.apache.spark.util.Utils$$anon$4.run(Utils.scala:173)
15/11/06 15:36:51 INFO SecurityManager: Changing view acls to: mc41946
15/11/06 15:36:51 INFO SecurityManager: Changing modify acls to: mc41946
15/11/06 15:36:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mc41946); users with modify permissions: Set(mc41946)
15/11/06 15:36:53 INFO Slf4jLogger: Slf4jLogger started
15/11/06 15:36:53 INFO Remoting: Starting remoting
15/11/06 15:36:53 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:61015]
15/11/06 15:36:53 INFO Utils: Successfully started service 'sparkDriver' on port 61015.
15/11/06 15:36:53 INFO SparkEnv: Registering MapOutputTracker
15/11/06 15:36:53 INFO SparkEnv: Registering BlockManagerMaster
15/11/06 15:36:53 INFO DiskBlockManager: Created local directory at C:\Users\mc41946\AppData\Local\Temp\spark-local-20151106153653-5362
15/11/06 15:36:53 INFO MemoryStore: MemoryStore started with capacity 947.7 MB
15/11/06 15:36:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/11/06 15:36:54 INFO HttpFileServer: HTTP File server directory is C:\Users\mc41946\AppData\Local\Temp\spark-9ff644d9-0270-4dd1-86c5-4316633684f7
15/11/06 15:36:54 INFO HttpServer: Starting HTTP Server
15/11/06 15:36:54 INFO Utils: Successfully started service 'HTTP file server' on port 61016.
15/11/06 15:36:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/11/06 15:36:55 INFO SparkUI: Started SparkUI at http://LAP-R9XN5X3.corp.pri:4040
15/11/06 15:36:56 INFO SparkContext: Added JAR target/eduonix_spark-deploy.jar at http://10.0.0.8:61016/jars/eduonix_spark-deploy.jar with timestamp 1446845816763
15/11/06 15:36:56 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:61015/user/HeartbeatReceiver
15/11/06 15:36:57 INFO NettyBlockTransferService: Server created on 61035
15/11/06 15:36:57 INFO BlockManagerMaster: Trying to register BlockManager
15/11/06 15:36:57 INFO BlockManagerMasterActor: Registering block manager localhost:61035 with 947.7 MB RAM, BlockManagerId(<driver>, localhost, 61035)
15/11/06 15:36:57 INFO BlockManagerMaster: Registered BlockManager
15/11/06 15:36:57 INFO SparkContext: Starting job: reduce at JavaSparkPi.java:80
15/11/06 15:36:58 INFO DAGScheduler: Got job 0 (reduce at JavaSparkPi.java:80) with 10 output partitions (allowLocal=false)
15/11/06 15:36:58 INFO DAGScheduler: Final stage: Stage 0(reduce at JavaSparkPi.java:80)
15/11/06 15:36:58 INFO DAGScheduler: Parents of final stage: List()
15/11/06 15:36:58 INFO DAGScheduler: Missing parents: List()
15/11/06 15:36:58 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73), which has no missing parents
15/11/06 15:36:58 INFO MemoryStore: ensureFreeSpace(2192) called with curMem=0, maxMem=993735475
15/11/06 15:36:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 947.7 MB)
15/11/06 15:36:58 INFO MemoryStore: ensureFreeSpace(1541) called with curMem=2192, maxMem=993735475
15/11/06 15:36:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1541.0 B, free 947.7 MB)
15/11/06 15:36:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:61035 (size: 1541.0 B, free: 947.7 MB)
15/11/06 15:36:58 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/11/06 15:36:58 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:838
15/11/06 15:36:58 INFO DAGScheduler: Submitting 10 missing tasks from Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73)
15/11/06 15:36:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks
15/11/06 15:36:58 WARN TaskSetManager: Stage 0 contains a task of very large size (977 KB). The maximum recommended task size is 100 KB.
15/11/06 15:36:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1001390 bytes)
15/11/06 15:36:58 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1001390 bytes)
15/11/06 15:36:58 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/11/06 15:36:58 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/11/06 15:36:58 INFO Executor: Fetching http://10.0.0.8:61016/jars/eduonix_spark-deploy.jar with timestamp 1446845816763
15/11/06 15:36:58 INFO Utils: Fetching http://10.0.0.8:61016/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp189260331050359726.tmp
15/11/06 15:37:00 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:867)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 15:37:00 INFO Executor: Fetching http://10.0.0.8:61016/jars/eduonix_spark-deploy.jar with timestamp 1446845816763
15/11/06 15:37:00 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
15/11/06 15:37:00 INFO Utils: Fetching http://10.0.0.8:61016/jars/eduonix_spark-deploy.jar to C:\Users\mc41946\AppData\Local\Temp\fetchFileTemp6568603039978296649.tmp
15/11/06 15:37:00 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1001390 bytes)
15/11/06 15:37:00 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/11/06 15:37:00 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:404)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:873)
	at org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:853)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:411)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

15/11/06 15:37:00 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
15/11/06 15:37:00 INFO TaskSchedulerImpl: Cancelling stage 0
15/11/06 15:37:00 INFO TaskSchedulerImpl: Stage 0 was cancelled
15/11/06 15:37:00 INFO DAGScheduler: Job 0 failed: reduce at JavaSparkPi.java:80, took 3.003314 s
15/11/06 15:37:00 INFO Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1)
15/11/06 15:37:00 INFO Executor: Executor is trying to kill task 2.0 in stage 0.0 (TID 2)
15/11/06 15:37:01 ERROR Utils: Exception while deleting Spark temp dir: C:\Users\mc41946\AppData\Local\Temp\spark-9ff644d9-0270-4dd1-86c5-4316633684f7
java.io.IOException: Failed to delete: C:\Users\mc41946\AppData\Local\Temp\spark-9ff644d9-0270-4dd1-86c5-4316633684f7
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:782)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1$$anonfun$apply$mcV$sp$2.apply(Utils.scala:177)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1$$anonfun$apply$mcV$sp$2.apply(Utils.scala:175)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply$mcV$sp(Utils.scala:175)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:173)
	at org.apache.spark.util.Utils$$anon$4$$anonfun$run$1.apply(Utils.scala:173)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1460)
	at org.apache.spark.util.Utils$$anon$4.run(Utils.scala:173)
15/11/06 16:06:11 INFO SecurityManager: Changing view acls to: mc41946
15/11/06 16:06:11 INFO SecurityManager: Changing modify acls to: mc41946
15/11/06 16:06:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(mc41946); users with modify permissions: Set(mc41946)
15/11/06 16:06:12 INFO Slf4jLogger: Slf4jLogger started
15/11/06 16:06:12 INFO Remoting: Starting remoting
15/11/06 16:06:12 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:61624]
15/11/06 16:06:12 INFO Utils: Successfully started service 'sparkDriver' on port 61624.
15/11/06 16:06:13 INFO SparkEnv: Registering MapOutputTracker
15/11/06 16:06:13 INFO SparkEnv: Registering BlockManagerMaster
15/11/06 16:06:13 INFO DiskBlockManager: Created local directory at C:\Users\mc41946\AppData\Local\Temp\spark-local-20151106160613-15c0
15/11/06 16:06:13 INFO MemoryStore: MemoryStore started with capacity 947.7 MB
15/11/06 16:06:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/11/06 16:06:14 INFO HttpFileServer: HTTP File server directory is C:\Users\mc41946\AppData\Local\Temp\spark-06b386c6-ea4c-4f60-8cc0-739c211245f3
15/11/06 16:06:14 INFO HttpServer: Starting HTTP Server
15/11/06 16:06:14 INFO Utils: Successfully started service 'HTTP file server' on port 61625.
15/11/06 16:06:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/11/06 16:06:14 INFO SparkUI: Started SparkUI at http://LAP-R9XN5X3.corp.pri:4040
15/11/06 16:06:14 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@LAP-R9XN5X3.corp.pri:61624/user/HeartbeatReceiver
15/11/06 16:06:15 INFO NettyBlockTransferService: Server created on 61644
15/11/06 16:06:15 INFO BlockManagerMaster: Trying to register BlockManager
15/11/06 16:06:15 INFO BlockManagerMasterActor: Registering block manager localhost:61644 with 947.7 MB RAM, BlockManagerId(<driver>, localhost, 61644)
15/11/06 16:06:15 INFO BlockManagerMaster: Registered BlockManager
15/11/06 16:06:16 INFO SparkContext: Starting job: reduce at JavaSparkPi.java:80
15/11/06 16:06:16 INFO DAGScheduler: Got job 0 (reduce at JavaSparkPi.java:80) with 10 output partitions (allowLocal=false)
15/11/06 16:06:16 INFO DAGScheduler: Final stage: Stage 0(reduce at JavaSparkPi.java:80)
15/11/06 16:06:16 INFO DAGScheduler: Parents of final stage: List()
15/11/06 16:06:16 INFO DAGScheduler: Missing parents: List()
15/11/06 16:06:16 INFO DAGScheduler: Submitting Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73), which has no missing parents
15/11/06 16:06:16 INFO MemoryStore: ensureFreeSpace(2192) called with curMem=0, maxMem=993735475
15/11/06 16:06:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.1 KB, free 947.7 MB)
15/11/06 16:06:16 INFO MemoryStore: ensureFreeSpace(1541) called with curMem=2192, maxMem=993735475
15/11/06 16:06:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1541.0 B, free 947.7 MB)
15/11/06 16:06:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:61644 (size: 1541.0 B, free: 947.7 MB)
15/11/06 16:06:16 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/11/06 16:06:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:838
15/11/06 16:06:16 INFO DAGScheduler: Submitting 10 missing tasks from Stage 0 (MappedRDD[1] at map at JavaSparkPi.java:73)
15/11/06 16:06:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks
15/11/06 16:06:16 WARN TaskSetManager: Stage 0 contains a task of very large size (977 KB). The maximum recommended task size is 100 KB.
15/11/06 16:06:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1001329 bytes)
15/11/06 16:06:17 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1001329 bytes)
15/11/06 16:06:17 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/11/06 16:06:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/11/06 16:06:17 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 727 bytes result sent to driver
15/11/06 16:06:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 727 bytes result sent to driver
15/11/06 16:06:17 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1001329 bytes)
15/11/06 16:06:17 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
15/11/06 16:06:17 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 557 ms on localhost (1/10)
15/11/06 16:06:17 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1001329 bytes)
15/11/06 16:06:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 864 ms on localhost (2/10)
15/11/06 16:06:17 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
15/11/06 16:06:17 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 727 bytes result sent to driver
15/11/06 16:06:17 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 727 bytes result sent to driver
15/11/06 16:06:17 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1001329 bytes)
15/11/06 16:06:17 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
15/11/06 16:06:17 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 727 bytes result sent to driver
15/11/06 16:06:17 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1001329 bytes)
15/11/06 16:06:17 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
15/11/06 16:06:17 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 524 ms on localhost (3/10)
15/11/06 16:06:17 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 457 ms on localhost (4/10)
15/11/06 16:06:18 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 727 bytes result sent to driver
15/11/06 16:06:18 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1001329 bytes)
15/11/06 16:06:18 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
15/11/06 16:06:18 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 219 ms on localhost (5/10)
15/11/06 16:06:18 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 727 bytes result sent to driver
15/11/06 16:06:18 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1001329 bytes)
15/11/06 16:06:18 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
15/11/06 16:06:18 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 220 ms on localhost (6/10)
15/11/06 16:06:18 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 727 bytes result sent to driver
15/11/06 16:06:18 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1001329 bytes)
15/11/06 16:06:18 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)
15/11/06 16:06:18 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 188 ms on localhost (7/10)
15/11/06 16:06:18 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 727 bytes result sent to driver
15/11/06 16:06:18 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1001329 bytes)
15/11/06 16:06:18 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)
15/11/06 16:06:18 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 189 ms on localhost (8/10)
15/11/06 16:06:18 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 130 ms on localhost (9/10)
15/11/06 16:06:18 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 727 bytes result sent to driver
15/11/06 16:06:18 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 119 ms on localhost (10/10)
15/11/06 16:06:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/11/06 16:06:18 INFO DAGScheduler: Stage 0 (reduce at JavaSparkPi.java:80) finished in 1.548 s
15/11/06 16:06:18 INFO DAGScheduler: Job 0 finished: reduce at JavaSparkPi.java:80, took 2.269467 s
15/11/06 16:06:18 INFO SparkUI: Stopped Spark web UI at http://LAP-R9XN5X3.corp.pri:4040
15/11/06 16:06:18 INFO DAGScheduler: Stopping DAGScheduler
15/11/06 16:06:19 INFO MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!
15/11/06 16:06:19 INFO MemoryStore: MemoryStore cleared
15/11/06 16:06:19 INFO BlockManager: BlockManager stopped
15/11/06 16:06:19 INFO BlockManagerMaster: BlockManagerMaster stopped
15/11/06 16:06:19 INFO SparkContext: Successfully stopped SparkContext
15/11/06 16:06:19 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
